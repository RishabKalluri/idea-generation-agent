Title: Chain-of-Verification for Reducing LLM Hallucinations

Problem: Large language models often generate plausible-sounding but factually incorrect information, especially for knowledge-intensive tasks.

Existing Methods: Current approaches include retrieval augmentation and fine-tuning on curated data, but these require external knowledge bases or expensive training.

Motivation: By having the model generate verification questions about its own output and then answer them independently, we can catch inconsistencies without external resources. This self-verification approach leverages the model's own knowledge more effectively.

Proposed Method: First, generate an initial response to a query. Then, automatically generate verification questions targeting factual claims in the response. Answer each verification question independently (without seeing the original response). Finally, compare answers to identify and correct inconsistencies.

Experiment Plan: Evaluate on fact-verification benchmarks like FEVER and knowledge-intensive QA datasets. Compare against baseline LLMs and retrieval-augmented methods. Measure both factual accuracy and hallucination rate.
